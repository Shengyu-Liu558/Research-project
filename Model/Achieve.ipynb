{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Achieve.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ddqIDbpz3Af2",
        "colab_type": "text"
      },
      "source": [
        "## 1 Download the packages and install"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NL3hOMFVEEba",
        "colab_type": "code",
        "outputId": "1b6c7109-5100-4d1d-fbfb-c0f08a17d398",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 191
        }
      },
      "source": [
        "pip install rank_bm25"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting rank_bm25\n",
            "  Downloading https://files.pythonhosted.org/packages/d2/e4/38d03d6d5e2deae8d2838b81d6ba2742475ced42045f5c46aeb00c5fb79c/rank_bm25-0.2.tar.gz\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from rank_bm25) (1.17.4)\n",
            "Building wheels for collected packages: rank-bm25\n",
            "  Building wheel for rank-bm25 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for rank-bm25: filename=rank_bm25-0.2-cp36-none-any.whl size=4162 sha256=08eaf7f2f622490070bd25cb5374b6885a090b9ed46e28ec1d80728d7d4a514f\n",
            "  Stored in directory: /root/.cache/pip/wheels/6f/0c/1f/78945dd6a5478bbcdb50d73ac96ae5af2ffcdfcd374fd9b1bf\n",
            "Successfully built rank-bm25\n",
            "Installing collected packages: rank-bm25\n",
            "Successfully installed rank-bm25-0.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nUgm91BDMdYd",
        "colab_type": "code",
        "outputId": "b7ddb371-b422-436f-9ffe-9622440535e4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        }
      },
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lRKcnwuv3SXJ",
        "colab_type": "text"
      },
      "source": [
        "## 2 Import the packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VsP9NpqKD6N0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import re\n",
        "import requests\n",
        "import warnings\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import networkx as nx\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from string import digits\n",
        "from sklearn import metrics\n",
        "from sklearn.svm import SVC\n",
        "from bs4 import BeautifulSoup\n",
        "from rank_bm25 import BM25Okapi\n",
        "from collections import Counter\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.wordnet import WordNetLemmatizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jmqec6zI28xq",
        "colab_type": "text"
      },
      "source": [
        "## 3 Achievement"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rFMxdnbu3z1m",
        "colab_type": "text"
      },
      "source": [
        "### 3.1 Calculate and get common words"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IvgiKs8aEIHc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_dictionary(my_wordlist):\n",
        "    word_count = {}\n",
        "\n",
        "    for word in my_wordlist:\n",
        "        if word in word_count:\n",
        "            word_count[word] += 1\n",
        "        else:\n",
        "            word_count[word] = 1\n",
        "    c = Counter(word_count)\n",
        "    # returns the most occurring elements\n",
        "    top = c.most_common(20)\n",
        "    print(\"The most frequent words are:\", top)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3AwCwPaj33IF",
        "colab_type": "text"
      },
      "source": [
        "### 3.2 Get the date"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "--QohjMEFKbL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def my_date(my_soup):\n",
        "    a = []\n",
        "    for each_text_date1 in my_soup.findAll('span', {'class': 'revDate'}):\n",
        "        a.append(each_text_date1.text)\n",
        "    for each_text_date2 in my_soup.findAll('div', {'class': 'pubdate'}, {'class': 'h-color--green'}):\n",
        "        a.append(each_text_date2.text)\n",
        "    for each_text_date3 in my_soup.findAll('span', {'id': 'last-reviewed-date'}):\n",
        "        a.append(each_text_date3.text)\n",
        "    for each_text_date4 in my_soup.findAll('class', {'id': 'css-12r7hj0'}):\n",
        "        a.append(each_text_date4.text)\n",
        "    for each_text_date5 in my_soup.findAll('dd', {'id': 'date_posted'}):\n",
        "        a.append(each_text_date5.text)\n",
        "\n",
        "\n",
        "    if a != []:\n",
        "        for s in a:\n",
        "            if s is not None:\n",
        "                print(\"Date:\", s)\n",
        "                split_list = []\n",
        "                m = re.split('[, -]', s)\n",
        "                for i in m:\n",
        "                    if i.startswith('20'):\n",
        "                        split_list.append(i)\n",
        "\n",
        "        if split_list[0] is None:\n",
        "            date = 2016.80882\n",
        "        else:\n",
        "            date = int(split_list[0])\n",
        "\n",
        "    if a == []:\n",
        "        print(\"Date:\", None)\n",
        "        date = 2016.80882\n",
        "    \n",
        "    return date"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9pLUlAHK37y5",
        "colab_type": "text"
      },
      "source": [
        "### 3.3 Get the author"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H89KJFyCFKgX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def my_Author(my_soup):\n",
        "    person_list = []\n",
        "    for each_text_writer1 in my_soup.findAll('a', {'class': 'person'}):\n",
        "        person_list.append(each_text_writer1.text)\n",
        "    if person_list == []:\n",
        "        print(\"Author:\", None)\n",
        "    if person_list != []:\n",
        "        print(\"Author:\", person_list)\n",
        "    return len(person_list)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ff0fkUuIHC4U",
        "colab_type": "text"
      },
      "source": [
        "### 3.4 Get the reference"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v6mjYEvRHCBP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def my_reference(my_soup):\n",
        "    reference_list = []\n",
        "    for each_text_ref in my_soup.findAll('div', {'class': 'reference'}):\n",
        "        reference_list.append(each_text_ref.text)\n",
        "    for each_text_ref in my_soup.findAll('span', {'class': 'sources'}):\n",
        "        reference_list.append(each_text_ref.text)\n",
        "    if reference_list == []:\n",
        "        print(\"The number of references is: \", None)\n",
        "    if reference_list != []:\n",
        "        print(\"The number of references is :\", len(reference_list))\n",
        "    return len(reference_list)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kGkdgqpI4BTM",
        "colab_type": "text"
      },
      "source": [
        "### 3.5 BM25 algorithm to get scores about content and problem relevance"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cnFZ9_tAGoFx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def Bm25(question,clean_data):\n",
        "  tokenized_corpus = [clean_data]\n",
        "  bm25 = BM25Okapi(tokenized_corpus)\n",
        "  query = question\n",
        "  tokenized_query = query.split(\" \")\n",
        "\n",
        "  doc_scores = abs(bm25.get_scores(tokenized_query))\n",
        "  print(\"The Bm25 score about \",\"'\", question,\"'\",\":\",doc_scores[0])\n",
        "  return doc_scores[0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oTUk9aj84eVi",
        "colab_type": "text"
      },
      "source": [
        "### 3.6 TF-IDF algorithm to get scores about content and problem relevance"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vpmFOO-gMXYZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def Tf_Idf(question,clean_data):\n",
        "    new_data = \" \".join(clean_data)\n",
        "    corpus = [new_data]  \n",
        "\n",
        "    vectorizer = CountVectorizer()  \n",
        "    X = vectorizer.fit_transform(corpus)\n",
        "    \n",
        "    query = question\n",
        "    tokenized_query = query.split(\" \")\n",
        "    myvocabulary = tokenized_query\n",
        "\n",
        "\n",
        "    transformer = TfidfTransformer()  \n",
        "\n",
        "    tfidf = transformer.fit_transform(X)  \n",
        "\n",
        "    word=vectorizer.get_feature_names()\n",
        "    weight=tfidf.toarray()  \n",
        "\n",
        "    for i in range(len(weight)):\n",
        "        total_score = 0\n",
        "        for j in range(len(word)):\n",
        "\n",
        "            if word[j] in myvocabulary:\n",
        "                total_score = total_score + weight[i][j] \n",
        "#                 print(word[j],weight[i][j])\n",
        "\n",
        "        print(\"The tf-idf total score about \",\"'\",question,\"'\", \":\", total_score)\n",
        "    return total_score"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XRuZn87FM9nR",
        "colab_type": "text"
      },
      "source": [
        "### 3.7 PageRank algorithm to get scores about url"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E6En1CgmNGOK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def my_pagerank(url, my_soup):\n",
        "    external_link = []\n",
        "    internal_link = []\n",
        "    new_link = []\n",
        "    d =[]\n",
        "    links = [a['href'] for a in my_soup.find_all('a', href=True)]\n",
        "    print(\"The number of links:\", len(links))\n",
        "    for link in links:\n",
        "        exter_link = re.findall('(https?://|//www)(?:[-\\w.]|(?:%[\\da-fA-F]{2}))+', link)\n",
        "        if exter_link:\n",
        "            external_link.append(link)\n",
        "        else:\n",
        "            internal_link.append(link)\n",
        "    print(\"The number of external links is: \", len(external_link))\n",
        "    # print(internal_link)\n",
        "    \n",
        "    for i in external_link:\n",
        "        ss = [url, i]\n",
        "        d.append(ss)\n",
        "\n",
        "    for m in internal_link:\n",
        "        ss = [m, url]\n",
        "        d.append(ss)\n",
        "\n",
        "    edges = d\n",
        "    nodes = []\n",
        "    for edge in edges:\n",
        "        if edge[0] not in nodes:\n",
        "            nodes.append(edge[0])\n",
        "        if edge[1] not in nodes:\n",
        "            nodes.append(edge[1])\n",
        "\n",
        "    N = len(nodes)\n",
        "\n",
        "    i = 0\n",
        "    node_to_num = {}\n",
        "    for node in nodes:\n",
        "        node_to_num[node] = i\n",
        "        i += 1\n",
        "    for edge in edges:\n",
        "        edge[0] = node_to_num[edge[0]]\n",
        "        edge[1] = node_to_num[edge[1]]\n",
        "\n",
        "    S = np.zeros([N, N]) # make the Matrix\n",
        "    for edge in edges:\n",
        "        S[edge[1], edge[0]] = 1\n",
        "\n",
        "    for j in range(N):\n",
        "        sum_of_col = sum(S[:, j])\n",
        "        for i in range(N):\n",
        "            if sum_of_col == 0:\n",
        "                S[i, j] = S[i, j]\n",
        "            else:\n",
        "                S[i, j] /= sum_of_col\n",
        "\n",
        "    alpha = 0.85\n",
        "    A = alpha * S + (1 - alpha) / N * np.ones([N, N])\n",
        "\n",
        "    P_n = np.ones(N) / N\n",
        "    P_n1 = np.zeros(N)\n",
        "\n",
        "    e = 100000\n",
        "    k = 0\n",
        "\n",
        "    while e > 0.00000001:\n",
        "        P_n1 = np.dot(A, P_n)\n",
        "        e = P_n1 - P_n\n",
        "        e = max(map(abs, e))\n",
        "        P_n = P_n1\n",
        "        k += 1\n",
        "    print(\" \")\n",
        "    print('The scores of PageRank:', P_n[0])\n",
        "\n",
        "    return len(external_link)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dITdkhnzZUTZ",
        "colab_type": "text"
      },
      "source": [
        "#### 3.7.1 Graph about relationship of nodes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LGgc2YobZUkV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        " def draw_my_PageRank(): \n",
        "    G = nx.DiGraph()\n",
        "    plt.figure(figsize=(20, 20))\n",
        "    new_edges = [[0, 'external_link 0'], [0, 'external_link 1'], [0, 'external_link 2'], [0, 'external_link 3'],\n",
        "             [0, 'external_link 4'], [0, 'external_link 5'], [0, 'external_link 6'], [0, 'external_link 7'],\n",
        "             [0, 'external_link 8'], [0, 'external_link 9'], ['internal 0', 0], ['internal 1', 0],\n",
        "             ['internal 2', 0], ['internal_3', 0], ['internal 4', 0]]\n",
        "    for edge in new_edges:\n",
        "\n",
        "        G.add_edge(edge[0], edge[1])\n",
        "\n",
        "    nx.draw(G, with_labels=True, node_size=500, edge_vmin=2, edge_vmax=3)\n",
        "\n",
        "    plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FtLGpFnP4kPn",
        "colab_type": "text"
      },
      "source": [
        "### 3.8 Content preprocessing and get link number"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wPcvgnJRFRbA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def my_features(q,url):\n",
        "    new_count = 0\n",
        "    my_wordlist = []\n",
        "    my_clean_list = []\n",
        "    my_source_code = requests.get(url).text\n",
        "    my_soup = BeautifulSoup(my_source_code, 'html.parser')\n",
        "    \n",
        "    for each_text_l in my_soup.findAll('li'):\n",
        "        content_l = each_text_l.text\n",
        "        words_l = content_l.lower().split()\n",
        "        for each_word_l in words_l:\n",
        "            my_wordlist.append(each_word_l)\n",
        "       \n",
        "    for word in my_wordlist:\n",
        "        symbols = '!&@#$%^&*()_-+={[}]|\\;:\"<>?/., ' # delete the Special symbol such as HTML \n",
        "        \n",
        "        for i in range(0, len(symbols)):\n",
        "            word = word.replace(symbols[i], '')\n",
        "\n",
        "        if len(word) > 0:\n",
        "            word1 = re.findall(r'[^\\*(\"&/:?\\\\|)<>]', word, re.S) # delete the \"()\" and \"&\"\n",
        "            word1 = \"\".join(word1)\n",
        "            word1 = re.sub(\"^\\d+\\s|\\s\\d+\\s|\\s\\d\\w\\d|\\s\\d+$\", \" \", word1) # delete the number in words\n",
        "            num = word1.isdigit()\n",
        "            if not num:\n",
        "                my_clean_list.append(word1) # delete the number in string\n",
        "    \n",
        "    q = re.findall(r'[^\\*(\"&/:?\\\\|)<>]', q, re.S) # delete the Special symbol such as \"?\" in question\n",
        "    q = \"\".join(q)\n",
        "    \n",
        "    my_clean_list = [w for w in my_clean_list if w not in stopwords.words(\"english\")] # delete the stopwords\n",
        "    my_clean_list = [WordNetLemmatizer().lemmatize(w) for w in my_clean_list] # standardize different variations and distortions of a word\n",
        "\n",
        "    \n",
        "    dataset1 = pd.read_csv('heart_word.csv')\n",
        "    X = dataset1.iloc[:, 0].values\n",
        "    X= [x.lower() for x in X]\n",
        "    for single_word in my_clean_list:\n",
        "        if single_word in X:\n",
        "            new_count += 1\n",
        "    print(\" \")\n",
        "    print(\"-----------------------------------------------------------------------------------------------------------------------\")\n",
        "    if new_count == 0:\n",
        "        print(\" \")\n",
        "        print(\"This webpage is not heart disease related webpage, so the final rank is 0\")\n",
        "        Predict_time, Predict_reference, Predict_external_link, Predict_count_words, Predict_Tf_Idf, Predict_Bm25 = 0,0,0,0,0,0\n",
        "    \n",
        "    if new_count != 0:\n",
        "        print(\"The number of academic words is \",new_count, \", this webpage is heart disease related webpage\")\n",
        "        print(\" \")\n",
        "        print(\"The result about this url and question: \")\n",
        "        print(\" \")\n",
        "        Predict_time = my_date(my_soup)\n",
        "        my_Author(my_soup)\n",
        "        Predict_reference = my_reference(my_soup)\n",
        "        print(\"The number of words:\", len(my_clean_list))\n",
        "        Predict_external_link = my_pagerank(url, my_soup)\n",
        "        Predict_Bm25 = Bm25(q, my_clean_list)\n",
        "        Predict_Tf_Idf = Tf_Idf(q, my_clean_list)\n",
        "        Predict_count_words = len(my_clean_list)\n",
        "        create_dictionary(my_clean_list)\n",
        "        print(\"-----------------------------------------------------------------------------------------------------------------------\")\n",
        "\n",
        "    return Predict_time, Predict_reference, Predict_external_link, Predict_count_words, Predict_Tf_Idf, Predict_Bm25 "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6e0zB7KBkgZi",
        "colab_type": "text"
      },
      "source": [
        "### 3.9 Predict final rank"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IQGeJGotkyAE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def my_score(q,url):\n",
        "    warnings.filterwarnings(\"ignore\")\n",
        "    test_data=[]\n",
        "    a,b,c,d,e,f = my_features(q,url)\n",
        " \n",
        "    if a != 0:\n",
        "\n",
        "        test_data = [[a,b,c,d,e,f]]\n",
        "        test_data = np.array(test_data)\n",
        "\n",
        "        dataset = pd.read_csv('test.csv')  \n",
        "        X = dataset.iloc[:, 0:-1].values\n",
        "        X= np.append(test_data,X,axis=0)\n",
        " \n",
        "        min_max_scaler = MinMaxScaler()\n",
        "        X = min_max_scaler.fit_transform(X)\n",
        "\n",
        "        mm = X[0]\n",
        "        mm = [mm.tolist()]\n",
        "        mm = np.array(mm)\n",
        "        \n",
        "        XX = np.delete(X, 0, 0)\n",
        "\n",
        "        Y = dataset.iloc[:, -1].values\n",
        "\n",
        "        data_train, data_test, label_train, label_test = train_test_split(XX, Y, test_size=0.1, random_state=0)\n",
        "\n",
        "        \n",
        "        new_svc = SVC(kernel='linear',C= 54)\n",
        "        svc_model = new_svc.fit(data_train,label_train)\n",
        "        \n",
        "        svc_pred = new_svc.predict(mm)\n",
        "        \n",
        "        print(\" \")\n",
        "        print(\"The rank of this webpage quality (The range of rank is 1 - 7):\")\n",
        "        print(\"The final rank of this webpage \", svc_pred[0])\n",
        "\n",
        "        if svc_pred[0] >= 6:\n",
        "            print(\"This quality of this health webpage is good\")\n",
        "        if 6 > svc_pred[0] >= 4:\n",
        "            print(\"This quality of this health webpage is general\")\n",
        "        if svc_pred[0] < 4:\n",
        "            print(\"This quality of this health webpage is bad\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oJ_jCFLn4vw5",
        "colab_type": "text"
      },
      "source": [
        "## 4 Main function"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ewM9h1x4L-3P",
        "colab_type": "text"
      },
      "source": [
        "Test question and Url:\n",
        "\n",
        "Q: What are some of the risk factors for heart disease\n",
        "\n",
        "1. https://www.webmd.com/heart-disease/risk-factors-for-heart-disease\n",
        "\n",
        "2. https://www.hri.org.nz/health/learn/cardiovascular-disease/atherosclerosis?gclid=Cj0KCQiA-4nuBRCnARIsAHwyuPrugm6XLQ10jbRWlTOJ2-rHFzdyLK8QFaIdpROz7e28iWSPQVlfP0IaAqjJEALw_wcB\n",
        "\n",
        "3. https://time.com/5388659/how-to-keep-your-heart-healthy/\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GZafWlg1FWOQ",
        "colab_type": "code",
        "outputId": "fc5edfd2-4af3-42c1-98b4-f8c0ce5d9046",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 453
        }
      },
      "source": [
        "if __name__ == '__main__':\n",
        "    q = input(\"Please input your question about health: \")\n",
        "    url1 = input(\"Please input the url about this question: \")\n",
        "    my_score(q,url1)\n",
        "#     draw_my_PageRank()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Please input your question about health: What are some of the risk factors for heart disease\n",
            "Please input the url about this question: https://www.webmd.com/heart-disease/risk-factors-for-heart-disease\n",
            " \n",
            "-----------------------------------------------------------------------------------------------------------------------\n",
            "The number of academic words is  29 , this webpage is heart disease related webpage\n",
            " \n",
            "The result about this url and question: \n",
            " \n",
            "Date:  on July 01, 2019\n",
            "Author: None\n",
            "The number of references is : 1\n",
            "The number of words: 1184\n",
            "The number of links: 315\n",
            "The number of external links is:  299\n",
            " \n",
            "The scores of PageRank: 1.0268368872138536e-09\n",
            "The Bm25 score about  ' What are some of the risk factors for heart disease ' : 1.9396105672078172\n",
            "The tf-idf total score about  ' What are some of the risk factors for heart disease ' : 0.920452743982664\n",
            "The most frequent words are: [('heart', 39), ('disease', 23), ('health', 21), ('webmd', 20), ('healthy', 19), ('cholesterol', 18), ('risk', 18), ('blood', 12), ('high', 11), ('diabetes', 10), ('diet', 10), ('exercise', 10), ('pressure', 10), ('drug', 9), ('weight', 9), ('view', 8), ('living', 8), ('care', 8), ('news', 8), ('attack', 8)]\n",
            "-----------------------------------------------------------------------------------------------------------------------\n",
            " \n",
            "The rank of this webpage quality (The range of rank is 1 - 7):\n",
            "The final rank of this webpage  5\n",
            "This quality of this health webpage is general\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}